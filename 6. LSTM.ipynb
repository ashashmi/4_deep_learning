{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a449c8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow\n",
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f159941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (batch_size=32,timesteps=1,features=1)))\n",
    "# keras.layers.GRU(units = 50, return_sequences = True, input_shape = (batch_size=32,timesteps=1,features=1)))\n",
    "# only 2 arguments in input shape indicate any batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "350ac3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c50ed3f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1986-03-13</th>\n",
       "      <td>0.088542</td>\n",
       "      <td>0.101563</td>\n",
       "      <td>0.088542</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.061751</td>\n",
       "      <td>1031788800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-14</th>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.102431</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.063956</td>\n",
       "      <td>308160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-17</th>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.103299</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.102431</td>\n",
       "      <td>0.065059</td>\n",
       "      <td>133171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-18</th>\n",
       "      <td>0.102431</td>\n",
       "      <td>0.103299</td>\n",
       "      <td>0.098958</td>\n",
       "      <td>0.099826</td>\n",
       "      <td>0.063405</td>\n",
       "      <td>67766400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1986-03-19</th>\n",
       "      <td>0.099826</td>\n",
       "      <td>0.100694</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.098090</td>\n",
       "      <td>0.062302</td>\n",
       "      <td>47894400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Open      High       Low     Close  Adj Close      Volume\n",
       "Date                                                                     \n",
       "1986-03-13  0.088542  0.101563  0.088542  0.097222   0.061751  1031788800\n",
       "1986-03-14  0.097222  0.102431  0.097222  0.100694   0.063956   308160000\n",
       "1986-03-17  0.100694  0.103299  0.100694  0.102431   0.065059   133171200\n",
       "1986-03-18  0.102431  0.103299  0.098958  0.099826   0.063405    67766400\n",
       "1986-03-19  0.099826  0.100694  0.097222  0.098090   0.062302    47894400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"MSFT.csv\", na_values=['null'], index_col='Date', parse_dates=True, infer_datetime_format=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ea7b16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-04-27</th>\n",
       "      <td>261.579987</td>\n",
       "      <td>263.190002</td>\n",
       "      <td>260.119995</td>\n",
       "      <td>261.970001</td>\n",
       "      <td>261.970001</td>\n",
       "      <td>31014200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-28</th>\n",
       "      <td>256.079987</td>\n",
       "      <td>256.540009</td>\n",
       "      <td>252.949997</td>\n",
       "      <td>254.559998</td>\n",
       "      <td>254.559998</td>\n",
       "      <td>46903100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-29</th>\n",
       "      <td>255.460007</td>\n",
       "      <td>256.100006</td>\n",
       "      <td>249.000000</td>\n",
       "      <td>252.509995</td>\n",
       "      <td>252.509995</td>\n",
       "      <td>40589000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-30</th>\n",
       "      <td>249.740005</td>\n",
       "      <td>253.080002</td>\n",
       "      <td>249.600006</td>\n",
       "      <td>252.179993</td>\n",
       "      <td>252.179993</td>\n",
       "      <td>30929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-03</th>\n",
       "      <td>253.399994</td>\n",
       "      <td>254.350006</td>\n",
       "      <td>251.119995</td>\n",
       "      <td>251.860001</td>\n",
       "      <td>251.860001</td>\n",
       "      <td>19598900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Open        High         Low       Close   Adj Close  \\\n",
       "Date                                                                     \n",
       "2021-04-27  261.579987  263.190002  260.119995  261.970001  261.970001   \n",
       "2021-04-28  256.079987  256.540009  252.949997  254.559998  254.559998   \n",
       "2021-04-29  255.460007  256.100006  249.000000  252.509995  252.509995   \n",
       "2021-04-30  249.740005  253.080002  249.600006  252.179993  252.179993   \n",
       "2021-05-03  253.399994  254.350006  251.119995  251.860001  251.860001   \n",
       "\n",
       "              Volume  \n",
       "Date                  \n",
       "2021-04-27  31014200  \n",
       "2021-04-28  46903100  \n",
       "2021-04-29  40589000  \n",
       "2021-04-30  30929200  \n",
       "2021-05-03  19598900  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6b353e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8857, 6)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1945a8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Target Variable\n",
    "high = df['High']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ab8716d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.101563 0.102431 0.103299 0.103299 0.100694 0.09809  0.097222 0.092882\n",
      " 0.092014 0.095486 0.096354 0.096354 0.095486 0.097222 0.098958 0.097222\n",
      " 0.097222 0.097222 0.09809  0.098958 0.101563 0.101563 0.100694 0.105035\n",
      " 0.105035 0.105035 0.102431 0.101563 0.100694 0.111979 0.121962 0.118924\n",
      " 0.118056 0.115451 0.111979 0.111979 0.110243 0.111979 0.111111 0.111111\n",
      " 0.111111 0.113715 0.112847 0.111979 0.112847 0.114583 0.111979 0.110243\n",
      " 0.110243 0.108507 0.109375 0.111111 0.114583 0.118924 0.123264 0.121528\n",
      " 0.118056 0.118924 0.118924 0.118924] 0.1189239993691444\n"
     ]
    }
   ],
   "source": [
    "# the following example is for 1 feature, 60 timesteps\n",
    "X = []\n",
    "y = []\n",
    "for i in range(60, df.shape[0]):\n",
    "    X.append(high.iloc[i-60:i])    # 0-59 rows\n",
    "    y.append(high[i])              # 60th row\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9cbc6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8797, 60)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80f8a661",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8797,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19df8ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "538a0bf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7037, 60)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4343d91a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1760, 60)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72033571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one more dimension to X_train i.e. (9023, 60) to (9023,60,1)\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b357643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "# LSTM input_shape(60,1) means number_of_timesteps=60, features=1)\n",
    "# input_shape(batch_size,timesteps,features), only 2 arguments indicate any batch size \n",
    "model.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50, return_sequences = True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units = 50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units = 1))     # as we are dealing with one feature at a time\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97a827bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 2444.2478\n",
      "Epoch 1: loss improved from inf to 2444.24780, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 17s 54ms/step - loss: 2444.2478\n",
      "Epoch 2/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 1973.8534\n",
      "Epoch 2: loss improved from 2444.24780 to 1973.85339, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 52ms/step - loss: 1973.8534\n",
      "Epoch 3/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 1697.7389\n",
      "Epoch 3: loss improved from 1973.85339 to 1697.73889, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 11s 52ms/step - loss: 1697.7389\n",
      "Epoch 4/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 1486.4294\n",
      "Epoch 4: loss improved from 1697.73889 to 1486.42944, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 11s 51ms/step - loss: 1486.4294\n",
      "Epoch 5/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 1310.4740\n",
      "Epoch 5: loss improved from 1486.42944 to 1307.54614, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 1307.5461\n",
      "Epoch 6/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 1169.6031\n",
      "Epoch 6: loss improved from 1307.54614 to 1169.60315, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 1169.6031\n",
      "Epoch 7/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 1040.4114\n",
      "Epoch 7: loss improved from 1169.60315 to 1038.64832, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 1038.6483\n",
      "Epoch 8/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 927.3233\n",
      "Epoch 8: loss improved from 1038.64832 to 927.32330, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 927.3233\n",
      "Epoch 9/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 832.3939\n",
      "Epoch 9: loss improved from 927.32330 to 832.39392, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 832.3939\n",
      "Epoch 10/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 736.5478\n",
      "Epoch 10: loss improved from 832.39392 to 736.54779, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 736.5478\n",
      "Epoch 11/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 648.4226\n",
      "Epoch 11: loss improved from 736.54779 to 648.42261, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 648.4226\n",
      "Epoch 12/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 588.7335\n",
      "Epoch 12: loss improved from 648.42261 to 588.73346, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 588.7335\n",
      "Epoch 13/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 513.2122\n",
      "Epoch 13: loss improved from 588.73346 to 513.21222, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 513.2122\n",
      "Epoch 14/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 462.4576\n",
      "Epoch 14: loss improved from 513.21222 to 460.58807, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 460.5881\n",
      "Epoch 15/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 413.1056\n",
      "Epoch 15: loss improved from 460.58807 to 413.10556, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 413.1056\n",
      "Epoch 16/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 372.8473\n",
      "Epoch 16: loss improved from 413.10556 to 373.56647, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 373.5665\n",
      "Epoch 17/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 336.1104\n",
      "Epoch 17: loss improved from 373.56647 to 336.11041, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 336.1104\n",
      "Epoch 18/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 294.1282\n",
      "Epoch 18: loss improved from 336.11041 to 294.12820, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 294.1282\n",
      "Epoch 19/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 263.3008\n",
      "Epoch 19: loss improved from 294.12820 to 263.30084, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 263.3008\n",
      "Epoch 20/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 230.3472\n",
      "Epoch 20: loss improved from 263.30084 to 230.34720, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 230.3472\n",
      "Epoch 21/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 213.0335\n",
      "Epoch 21: loss improved from 230.34720 to 213.03348, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 213.0335\n",
      "Epoch 22/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 181.7999\n",
      "Epoch 22: loss improved from 213.03348 to 181.79987, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 181.7999\n",
      "Epoch 23/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 166.9343\n",
      "Epoch 23: loss improved from 181.79987 to 166.93431, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 166.9343\n",
      "Epoch 24/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 148.4146\n",
      "Epoch 24: loss improved from 166.93431 to 148.41460, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 148.4146\n",
      "Epoch 25/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 137.0234\n",
      "Epoch 25: loss improved from 148.41460 to 138.39200, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 138.3920\n",
      "Epoch 26/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 120.3938\n",
      "Epoch 26: loss improved from 138.39200 to 120.39381, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 120.3938\n",
      "Epoch 27/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 105.1246\n",
      "Epoch 27: loss improved from 120.39381 to 104.79015, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 104.7901\n",
      "Epoch 28/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 94.7868\n",
      "Epoch 28: loss improved from 104.79015 to 94.78684, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 94.7868\n",
      "Epoch 29/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 86.4923\n",
      "Epoch 29: loss improved from 94.78684 to 86.49231, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 86.4923\n",
      "Epoch 30/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 75.1458\n",
      "Epoch 30: loss improved from 86.49231 to 75.14583, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 75.1458\n",
      "Epoch 31/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 64.2435\n",
      "Epoch 31: loss improved from 75.14583 to 64.24353, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 64.2435\n",
      "Epoch 32/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 57.6389\n",
      "Epoch 32: loss improved from 64.24353 to 57.63892, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 57.6389\n",
      "Epoch 33/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 56.6159\n",
      "Epoch 33: loss improved from 57.63892 to 56.61592, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 56.6159\n",
      "Epoch 34/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 50.4357\n",
      "Epoch 34: loss improved from 56.61592 to 50.34938, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 50.3494\n",
      "Epoch 35/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 44.8576\n",
      "Epoch 35: loss improved from 50.34938 to 44.85764, saving model to lstm_stock.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 12s 54ms/step - loss: 44.8576\n",
      "Epoch 36/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 44.8988\n",
      "Epoch 36: loss did not improve from 44.85764\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 44.8988\n",
      "Epoch 37/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 39.6423\n",
      "Epoch 37: loss improved from 44.85764 to 39.64232, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 39.6423\n",
      "Epoch 38/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 38.4015\n",
      "Epoch 38: loss improved from 39.64232 to 38.40149, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 38.4015\n",
      "Epoch 39/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 37.4402\n",
      "Epoch 39: loss improved from 38.40149 to 37.44020, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 37.4402\n",
      "Epoch 40/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 31.6542\n",
      "Epoch 40: loss improved from 37.44020 to 31.65425, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 31.6542\n",
      "Epoch 41/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 29.7534\n",
      "Epoch 41: loss improved from 31.65425 to 29.75344, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 29.7534\n",
      "Epoch 42/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 29.8562\n",
      "Epoch 42: loss did not improve from 29.75344\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 29.8562\n",
      "Epoch 43/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 27.2753\n",
      "Epoch 43: loss improved from 29.75344 to 27.27531, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 27.2753\n",
      "Epoch 44/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 28.7333\n",
      "Epoch 44: loss did not improve from 27.27531\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 28.7333\n",
      "Epoch 45/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 26.3704\n",
      "Epoch 45: loss improved from 27.27531 to 26.37043, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 26.3704\n",
      "Epoch 46/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 27.7097\n",
      "Epoch 46: loss did not improve from 26.37043\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 27.7097\n",
      "Epoch 47/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 25.9462\n",
      "Epoch 47: loss improved from 26.37043 to 25.94625, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 25.9462\n",
      "Epoch 48/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 26.9469\n",
      "Epoch 48: loss did not improve from 25.94625\n",
      "220/220 [==============================] - 12s 53ms/step - loss: 26.9469\n",
      "Epoch 49/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 26.6434\n",
      "Epoch 49: loss did not improve from 25.94625\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 26.6434\n",
      "Epoch 50/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 25.1998\n",
      "Epoch 50: loss improved from 25.94625 to 25.19984, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 25.1998\n",
      "Epoch 51/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 27.7494\n",
      "Epoch 51: loss did not improve from 25.19984\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 27.7494\n",
      "Epoch 52/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 29.4983\n",
      "Epoch 52: loss did not improve from 25.19984\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 29.4983\n",
      "Epoch 53/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 32.9988\n",
      "Epoch 53: loss did not improve from 25.19984\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 32.9988\n",
      "Epoch 54/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 27.2814\n",
      "Epoch 54: loss did not improve from 25.19984\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 27.2814\n",
      "Epoch 55/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.2683\n",
      "Epoch 55: loss improved from 25.19984 to 24.26826, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 24.2683\n",
      "Epoch 56/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.4239\n",
      "Epoch 56: loss did not improve from 24.26826\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 24.4239\n",
      "Epoch 57/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 25.3052\n",
      "Epoch 57: loss did not improve from 24.26826\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 25.3052\n",
      "Epoch 58/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 26.7823\n",
      "Epoch 58: loss did not improve from 24.26826\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 26.7823\n",
      "Epoch 59/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.4796\n",
      "Epoch 59: loss improved from 24.26826 to 23.47960, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.4796\n",
      "Epoch 60/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.3788\n",
      "Epoch 60: loss improved from 23.47960 to 23.37881, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.3788\n",
      "Epoch 61/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.0957\n",
      "Epoch 61: loss improved from 23.37881 to 23.09574, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.0957\n",
      "Epoch 62/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.8322\n",
      "Epoch 62: loss did not improve from 23.09574\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.8322\n",
      "Epoch 63/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.7128\n",
      "Epoch 63: loss did not improve from 23.09574\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.7128\n",
      "Epoch 64/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 25.3374\n",
      "Epoch 64: loss did not improve from 23.09574\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 25.3374\n",
      "Epoch 65/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.9905\n",
      "Epoch 65: loss did not improve from 23.09574\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.9905\n",
      "Epoch 66/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 25.6222\n",
      "Epoch 66: loss did not improve from 23.09574\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 25.6222\n",
      "Epoch 67/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 25.0120\n",
      "Epoch 67: loss did not improve from 23.09574\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 25.0120\n",
      "Epoch 68/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 22.3180\n",
      "Epoch 68: loss improved from 23.09574 to 22.31797, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 22.3180\n",
      "Epoch 69/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 22.5292\n",
      "Epoch 69: loss did not improve from 22.31797\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 22.5292\n",
      "Epoch 70/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.3358\n",
      "Epoch 70: loss did not improve from 22.31797\n",
      "220/220 [==============================] - 12s 57ms/step - loss: 24.3358\n",
      "Epoch 71/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.3815\n",
      "Epoch 71: loss did not improve from 22.31797\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.3815\n",
      "Epoch 72/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.9727\n",
      "Epoch 72: loss did not improve from 22.31797\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 24.9727\n",
      "Epoch 73/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.0159\n",
      "Epoch 73: loss did not improve from 22.31797\n",
      "220/220 [==============================] - 12s 56ms/step - loss: 23.0159\n",
      "Epoch 74/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 26.8062\n",
      "Epoch 74: loss did not improve from 22.31797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 12s 55ms/step - loss: 26.8062\n",
      "Epoch 75/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 22.1447\n",
      "Epoch 75: loss improved from 22.31797 to 22.30469, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 22.3047\n",
      "Epoch 76/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 22.2480\n",
      "Epoch 76: loss improved from 22.30469 to 22.24800, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 55ms/step - loss: 22.2480\n",
      "Epoch 77/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 26.1208\n",
      "Epoch 77: loss did not improve from 22.24800\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 26.1208\n",
      "Epoch 78/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 22.3527\n",
      "Epoch 78: loss did not improve from 22.24800\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 22.3527\n",
      "Epoch 79/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 22.1507\n",
      "Epoch 79: loss improved from 22.24800 to 22.15068, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 22.1507\n",
      "Epoch 80/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 21.9706\n",
      "Epoch 80: loss improved from 22.15068 to 21.97063, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 21.9706\n",
      "Epoch 81/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.1985\n",
      "Epoch 81: loss did not improve from 21.97063\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 23.1985\n",
      "Epoch 82/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.0329\n",
      "Epoch 82: loss did not improve from 21.97063\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 24.0329\n",
      "Epoch 83/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 21.9550\n",
      "Epoch 83: loss improved from 21.97063 to 21.95504, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 12s 54ms/step - loss: 21.9550\n",
      "Epoch 84/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 23.5745\n",
      "Epoch 84: loss did not improve from 21.95504\n",
      "220/220 [==============================] - 11s 52ms/step - loss: 23.6161\n",
      "Epoch 85/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 21.3752\n",
      "Epoch 85: loss improved from 21.95504 to 21.32822, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 21.3282\n",
      "Epoch 86/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.1190\n",
      "Epoch 86: loss did not improve from 21.32822\n",
      "220/220 [==============================] - 10s 46ms/step - loss: 23.1190\n",
      "Epoch 87/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.9610\n",
      "Epoch 87: loss did not improve from 21.32822\n",
      "220/220 [==============================] - 10s 45ms/step - loss: 24.9610\n",
      "Epoch 88/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.5207\n",
      "Epoch 88: loss did not improve from 21.32822\n",
      "220/220 [==============================] - 10s 45ms/step - loss: 23.5207\n",
      "Epoch 89/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 21.5372\n",
      "Epoch 89: loss did not improve from 21.32822\n",
      "220/220 [==============================] - 10s 43ms/step - loss: 21.5372\n",
      "Epoch 90/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 24.5646\n",
      "Epoch 90: loss did not improve from 21.32822\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 24.5646\n",
      "Epoch 91/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 19.4398\n",
      "Epoch 91: loss improved from 21.32822 to 19.42922, saving model to lstm_stock.h5\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 19.4292\n",
      "Epoch 92/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.9732\n",
      "Epoch 92: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 45ms/step - loss: 23.9732\n",
      "Epoch 93/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 22.0143\n",
      "Epoch 93: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 45ms/step - loss: 21.9669\n",
      "Epoch 94/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 22.6627\n",
      "Epoch 94: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 22.6114\n",
      "Epoch 95/100\n",
      "220/220 [==============================] - ETA: 0s - loss: 23.4977\n",
      "Epoch 95: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 23.4977\n",
      "Epoch 96/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 20.9315\n",
      "Epoch 96: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 21.1929\n",
      "Epoch 97/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 20.1415\n",
      "Epoch 97: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 20.1291\n",
      "Epoch 98/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 21.0872\n",
      "Epoch 98: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 21.2078\n",
      "Epoch 99/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 21.9484\n",
      "Epoch 99: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 21.9019\n",
      "Epoch 100/100\n",
      "219/220 [============================>.] - ETA: 0s - loss: 22.8733\n",
      "Epoch 100: loss did not improve from 19.42922\n",
      "220/220 [==============================] - 10s 44ms/step - loss: 23.0900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23e49b136a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# ModelCheckpoint saves the model while monitoring a specific parameter of the model. \n",
    "# In this case we are monitoring loss. \n",
    "# The model will only be saved if the loss in current epoch is less than in the last epoch.\n",
    "checkpoint = ModelCheckpoint(\"lstm_stock.h5\", monitor='loss', verbose=1, \n",
    "                             save_best_only=True, save_weights_only=False, mode='auto')\n",
    "\n",
    "# EarlyStopping stops training of the model early if there is no increase in the monitored parameter. \n",
    "# In this case we are monitoring loss. \n",
    "# patience=20 means that model will stop to train if it doesnâ€™t see any decrease in loss in 20 epochs.\n",
    "early = EarlyStopping(monitor='loss', min_delta=0, patience=20, verbose=1, mode='auto')\n",
    "\n",
    "model.fit(X_train, y_train, epochs = 100, batch_size = 32, callbacks=[checkpoint,early])      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88b681e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55/55 [==============================] - 1s 13ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.0618515],\n",
       "       [ 1.9811678],\n",
       "       [27.031998 ],\n",
       "       ...,\n",
       "       [17.78685  ],\n",
       "       [ 8.598194 ],\n",
       "       [23.149738 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d15c24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
